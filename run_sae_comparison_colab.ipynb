{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d627381d",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "d627381d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9b7f1813",
      "metadata": {
        "id": "9b7f1813"
      },
      "source": [
        "\n",
        "# SAE Comparison Experiment - Google Colab\n",
        "### This notebook runs your SAE comparison experiment with the existing code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5e8a508",
      "metadata": {
        "id": "c5e8a508"
      },
      "source": [
        "### 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fef4534f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fef4534f",
        "outputId": "20dd9818-5a97-438c-f7b8-f5d183a104c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "Cloning into 'repo'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 37 (delta 3), reused 25 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (37/37), 36.94 KiB | 945.00 KiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "/content/repo/repo/repo\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Check GPU\n",
        "!nvidia-smi\n",
        "\n",
        "# %%\n",
        "# Clone your repository (replace with your actual repo URL)\n",
        "!git clone https://github.com/StrikerAI-Innovation/mechanistic-interpretability-scale.git repo\n",
        "%cd repo\n",
        "\n",
        "# %%\n",
        "# Install required packages\n",
        "!pip install torch transformers datasets accelerate -q\n",
        "!pip install numpy pandas matplotlib seaborn tqdm pyyaml -q\n",
        "!pip install einops wandb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c28f8c",
      "metadata": {
        "id": "b7c28f8c"
      },
      "source": [
        "### 2. Import and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9b9c3ce8",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b9c3ce8",
        "outputId": "56526a09-0027-4ef0-b4b0-0fc0ecf25324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Import error: No module named 'src.models.gated_sae'\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "# Test imports\n",
        "try:\n",
        "    from src.models.base_sae import BaseSAE\n",
        "    from src.models.k_sparse_sae import KSparseSAE\n",
        "    from src.models.hybrid_sae import HybridSAE\n",
        "    from src.models.vanilla_sae import VanillaSAE\n",
        "    from src.training.trainer import SAETrainer\n",
        "    from src.utils.data_loading import create_dataloader\n",
        "    from src.utils.model_loading import load_model\n",
        "    print(\"✓ All imports successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Import error: {e}\")\n",
        "\n",
        "# %%\n",
        "# Check PyTorch and GPU\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c27a945f",
      "metadata": {
        "id": "c27a945f"
      },
      "source": [
        "### 3. Create Simplified Config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c69cb4",
      "metadata": {
        "id": "43c69cb4"
      },
      "source": [
        "**bold text**# %%\n",
        "# Create a Colab-friendly config\n",
        "import yaml\n",
        "\n",
        "colab_config = {\n",
        "    'seed': 42,\n",
        "    'dataset': 'wikitext',\n",
        "    'max_train_samples': 5000,   # Reduced for Colab\n",
        "    'max_val_samples': 1000,     # Reduced for Colab\n",
        "    'k_sparse': 64,              # Reduced from 128\n",
        "    'training': {\n",
        "        'batch_size': 16,        # Reduced from 32\n",
        "        'epochs': 5,             # Reduced from 10\n",
        "        'learning_rate': 1e-3,\n",
        "        'weight_decay': 1e-4,\n",
        "        'warmup_steps': 500,\n",
        "        'max_grad_norm': 1.0,\n",
        "        'scheduler_type': 'cosine'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save config\n",
        "os.makedirs('experiments/01_sae_comparison', exist_ok=True)\n",
        "with open('experiments/01_sae_comparison/colab_config.yaml', 'w') as f:\n",
        "    yaml.dump(colab_config, f)\n",
        "\n",
        "print(\"Config created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Colab-friendly config\n",
        "import yaml\n",
        "\n",
        "colab_config = {\n",
        "    'seed': 42,\n",
        "    'dataset': 'wikitext',\n",
        "    'max_train_samples': 5000,   # Reduced for Colab\n",
        "    'max_val_samples': 1000,     # Reduced for Colab\n",
        "    'k_sparse': 64,              # Reduced from 128\n",
        "    'training': {\n",
        "        'batch_size': 16,        # Reduced from 32\n",
        "        'epochs': 5,             # Reduced from 10\n",
        "        'learning_rate': 1e-3,\n",
        "        'weight_decay': 1e-4,\n",
        "        'warmup_steps': 500,\n",
        "        'max_grad_norm': 1.0,\n",
        "        'scheduler_type': 'cosine'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save config\n",
        "os.makedirs('experiments/01_sae_comparison', exist_ok=True)\n",
        "with open('experiments/01_sae_comparison/colab_config.yaml', 'w') as f:\n",
        "    yaml.dump(colab_config, f)\n",
        "\n",
        "print(\"Config created!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsmgUWauTdNc",
        "outputId": "a7e135cf-d49b-4906-a328-3d2515b678c4"
      },
      "id": "hsmgUWauTdNc",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a681fea",
      "metadata": {
        "id": "8a681fea"
      },
      "source": [
        "### 4. Run Quick Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "051630c4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "051630c4",
        "outputId": "8d56f41b-faf0-491d-f81a-28015b66c747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/repo/experiments/01_sae_comparison/run_comparison.py\", line 28, in <module>\n",
            "    from src.models import KSparseSAE, GatedSAE, HybridSAE\n",
            "ImportError: cannot import name 'KSparseSAE' from 'src.models' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/repo/experiments/01_sae_comparison/run_comparison.py\", line 16, in <module>\n",
            "    import matplotlib.pyplot as plt\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\", line 3774, in <module>\n",
            "    @_copy_docstring_and_deprecators(Axes.pie)\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\", line 196, in _copy_docstring_and_deprecators\n",
            "    _add_pyplot_note(func, method)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\", line 254, in _add_pyplot_note\n",
            "    doc = inspect.cleandoc(func.__doc__)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 884, in cleandoc\n",
            "    for i in range(1, len(lines)): lines[i] = lines[i][margin:]\n",
            "                                   ~~~~~^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Quick test to make sure everything works\n",
        "!python experiments/01_sae_comparison/run_comparison.py \\\n",
        "    --model gpt2 \\\n",
        "    --layer 6 \\\n",
        "    --n_features 4096 \\\n",
        "    --batch_size 8 \\\n",
        "    --epochs 2 \\\n",
        "    --debug \\\n",
        "    --config experiments/01_sae_comparison/colab_config.yaml\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Run Main Experiment\n",
        "\n",
        "# %%\n",
        "# Run the full comparison (this will take 1-3 hours)\n",
        "!python experiments/01_sae_comparison/run_comparison.py \\\n",
        "    --model gpt2 \\\n",
        "    --layer 6 \\\n",
        "    --n_features 8192 \\\n",
        "    --batch_size 16 \\\n",
        "    --epochs 5 \\\n",
        "    --config experiments/01_sae_comparison/colab_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faa53982",
      "metadata": {
        "id": "faa53982"
      },
      "source": [
        "### 6. Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eee059f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "3eee059f"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# Load and display results\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Find the latest results directory\n",
        "results_dirs = sorted(Path('results').glob('sae_comparison_*'))\n",
        "if results_dirs:\n",
        "    latest_results = results_dirs[-1]\n",
        "    print(f\"Results directory: {latest_results}\")\n",
        "\n",
        "    # Load summary\n",
        "    if (latest_results / 'summary_results.csv').exists():\n",
        "        summary_df = pd.read_csv(latest_results / 'summary_results.csv')\n",
        "        print(\"\\nSummary Results:\")\n",
        "        display(summary_df)\n",
        "\n",
        "    # Display plots\n",
        "    for img_path in latest_results.glob('*.png'):\n",
        "        print(f\"\\n{img_path.name}:\")\n",
        "        display(Image(str(img_path)))\n",
        "else:\n",
        "    print(\"No results found yet!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf6eab2",
      "metadata": {
        "id": "9bf6eab2"
      },
      "source": [
        "### 7. Save to Google Drive (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fadc1d92",
      "metadata": {
        "id": "fadc1d92"
      },
      "source": [
        "# %%\n",
        "# Mount Google Drive to save results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# %%\n",
        "# Copy results to Drive\n",
        "import shutil\n",
        "\n",
        "if results_dirs:\n",
        "    save_path = '/content/drive/MyDrive/sae_results'\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    \n",
        "    # Copy latest results\n",
        "    dest = os.path.join(save_path, latest_results.name)\n",
        "    shutil.copytree(latest_results, dest, dirs_exist_ok=True)\n",
        "    print(f\"Results saved to: {dest}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Alternative: Run Without Config File\n",
        "\n",
        "# %%\n",
        "# If config file approach doesn't work, run directly with all arguments\n",
        "!python experiments/01_sae_comparison/run_comparison.py \\\n",
        "    --model gpt2 \\\n",
        "    --layer 6 \\\n",
        "    --n_features 8192 \\\n",
        "    --batch_size 16 \\\n",
        "    --epochs 5 \\\n",
        "    --learning_rate 1e-3 \\\n",
        "    --seed 42"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15fa0b1a",
      "metadata": {
        "id": "15fa0b1a"
      },
      "source": [
        "### Memory Management (If Needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04e4ccd7",
      "metadata": {
        "id": "04e4ccd7"
      },
      "source": [
        "# %%\n",
        "# If you run into memory issues, try these settings\n",
        "!python experiments/01_sae_comparison/run_comparison.py \\\n",
        "    --model gpt2 \\\n",
        "    --layer 6 \\\n",
        "    --n_features 4096 \\\n",
        "    --batch_size 8 \\\n",
        "    --epochs 3 \\\n",
        "    --config experiments/01_sae_comparison/colab_config.yaml\n",
        "\n",
        "# %%\n",
        "# Clear GPU memory if needed\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "clear_memory()\n",
        "print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}